{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Compression\n",
    "\n",
    "- [Tutorial](https://github.com/hyperopt/hyperopt/wiki/FMin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin\n",
    "import hyperopt\n",
    "import math\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "# script_dir = os.path.dirname(__file__)\n",
    "# module_path = os.path.abspath(os.path.join(script_dir, '..', '..'))\n",
    "# try:\n",
    "#     import distiller\n",
    "# except ImportError:\n",
    "#     sys.path.append(module_path)\n",
    "#     import distiller\n",
    "import distiller\n",
    "import apputils\n",
    "from models import ALL_MODEL_NAMES, create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_range(val_str):\n",
    "    val = float(val_str)\n",
    "    if val < 0 or val >= 1:\n",
    "        raise argparse.ArgumentTypeError('Must be >= 0 and < 1 (received {0})'.format(val_str))\n",
    "    return val\n",
    "\n",
    "# We only change the pruning hyperparameter for convolution layer, \n",
    "# and the value ranges from 0.01 to 0.99 \n",
    "def get_space():\n",
    "    space = {}\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if 'conv' in name and 'weight' in name:\n",
    "            space[name] = hp.uniform(name, 0.01, 0.99)\n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Distiller image classification model compression')\n",
    "parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet20_cifar',\n",
    "                    choices=ALL_MODEL_NAMES,\n",
    "                    help='model architecture: ' +\n",
    "                    ' | '.join(ALL_MODEL_NAMES) +\n",
    "                    ' (default: resnet20_cifar)')\n",
    "parser.add_argument('-r', '--rounds', default=10, type=int,\n",
    "                    metavar='R', help='max rounds (default: 10)')\n",
    "parser.add_argument('--epochs', default=120, type=int,\n",
    "                    metavar='E', help='epochs (default: 120)')\n",
    "parser.add_argument('-j', '--workers', default=1, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 1)')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 128)')\n",
    "parser.add_argument('--gpus', metavar='DEV_ID', default=None,\n",
    "                    help='Comma-separated list of GPU device IDs to be used (default is to use all available devices)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--validation-size', '--vs', type=float_range, default=0.1,\n",
    "                    help='Portion of training dataset to set aside for validation')\n",
    "parser.add_argument('--deterministic', '--det', action='store_true',\n",
    "                    help='Ensure deterministic execution for re-producible results.')\n",
    "\n",
    "# [Manual setting hyperparameters]\n",
    "# If execute in command line, use the following line instead:\n",
    "#    args = parser.parse_args()  \n",
    "args = parser.parse_args(args=[\n",
    "    '/tmp/dataset-nctu',\n",
    "    '-a', 'resnet56_cifar',\n",
    "    '--gpus', '0'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "args.dataset = 'cifar10' if 'cifar' in args.arch else 'imagenet'\n",
    "if args.gpus is not None:\n",
    "    try:\n",
    "        args.gpus = [int(s) for s in args.gpus.split(',')]\n",
    "    except ValueError:\n",
    "        exit(1)\n",
    "    available_gpus = torch.cuda.device_count()\n",
    "    for dev_id in args.gpus:\n",
    "        if dev_id >= available_gpus:\n",
    "            exit(1)\n",
    "    # Set default device in case the first one on the list != 0\n",
    "    torch.cuda.set_device(args.gpus[0])\n",
    "\n",
    "model = create_model(False, args.dataset, args.arch, device_ids=args.gpus) # Get arch state_dict\n",
    "train_loader, val_loader, test_loader, _ = apputils.load_data(\n",
    "        args.dataset, os.path.expanduser(args.data), args.batch_size,\n",
    "        args.workers, args.validation_size, args.deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, criterion, optimizer, compression_scheduler):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_samples = len(train_loader.sampler)\n",
    "    batch_size = train_loader.batch_size\n",
    "    steps_per_epoch = math.ceil(total_samples / batch_size)\n",
    "    for train_step, (inputs, targets) in enumerate(train_loader):\n",
    "        compression_scheduler.on_minibatch_begin(epoch, train_step, steps_per_epoch, optimizer)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        outputs = model(inputs.cuda())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum().data.numpy()\n",
    "        loss = criterion(outputs, targets)\n",
    "        compression_scheduler.before_backward_pass(epoch, train_step, steps_per_epoch, loss,\n",
    "                                                   optimizer=optimizer, return_loss_components=True)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        compression_scheduler.on_minibatch_end(epoch, train_step, steps_per_epoch, optimizer)\n",
    "    accuracy = 100. * correct / total    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_step, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum().data.numpy()\n",
    "    accuracy = 100. * correct / total    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for test_step, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum().data.numpy()\n",
    "    accuracy = 100. * correct / total    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    global model\n",
    "    global count\n",
    "    global outputDir\n",
    "    global f\n",
    "    #Explore new model\n",
    "    model = create_model(False, args.dataset, args.arch, device_ids=args.gpus)\n",
    "    count += 1\n",
    "    # Objective function: F(Acc, Lat) = (1 - Acc.) + (alpha * Sparsity)\n",
    "    accuracy = 0\n",
    "    alpha = 0.2 # Super-parameter: the importance of inference time\n",
    "    latency = 0.0\n",
    "    sparsity = 0.0\n",
    "    # Training hyperparameter\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "    \"\"\"\n",
    "    distiller/distiller/config.py\n",
    "        # Element-wise sparsity\n",
    "        sparsity_levels = {net_param: sparsity_level}\n",
    "        pruner = distiller.pruning.SparsityLevelParameterPruner(name='sensitivity', levels=sparsity_levels)\n",
    "        policy = distiller.PruningPolicy(pruner, pruner_args=None)\n",
    "        scheduler = distiller.CompressionScheduler(model)\n",
    "        scheduler.add_policy(policy, epochs=[0, 2, 4])\n",
    "        # Local search \n",
    "        add multiple pruner for each layer\n",
    "    \"\"\"\n",
    "    sparsity_levels = {}\n",
    "    for key, value in space.items():\n",
    "        sparsity_levels[key] = value\n",
    "    pruner = distiller.pruning.SparsityLevelParameterPruner(name='sensitivity', levels=sparsity_levels)\n",
    "    policy = distiller.PruningPolicy(pruner, pruner_args=None)\n",
    "    lrpolicy = distiller.LRPolicy(torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1))\n",
    "    compression_scheduler = distiller.CompressionScheduler(model)\n",
    "    #compression_scheduler.add_policy(policy, epochs=[90])    \n",
    "    compression_scheduler.add_policy(policy, starting_epoch=0, ending_epoch=90, frequency=1)\n",
    "    compression_scheduler.add_policy(lrpolicy, starting_epoch=0, ending_epoch=90, frequency=1)\n",
    "    \"\"\"\n",
    "    distiller/example/classifier_compression/compress_classifier.py\n",
    "    For each epoch:\n",
    "        compression_scheduler.on_epoch_begin(epoch)\n",
    "        train()\n",
    "        save_checkpoint()\n",
    "        compression_scheduler.on_epoch_end(epoch)\n",
    "\n",
    "    train():\n",
    "        For each training step:\n",
    "            compression_scheduler.on_minibatch_begin(epoch)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            compression_scheduler.before_backward_pass(epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            compression_scheduler.on_minibatch_end(epoch)\n",
    "    \"\"\"\n",
    "    for i in range(args.epochs):\n",
    "        compression_scheduler.on_epoch_begin(i)\n",
    "        train_accuracy = train(i,criterion, optimizer, compression_scheduler)\n",
    "        val_accuracy = validate() # Validate hyperparameter setting\n",
    "        t, sparsity = distiller.weights_sparsity_tbl_summary(model, return_total_sparsity=True)\n",
    "        compression_scheduler.on_epoch_end(i, optimizer)\n",
    "        apputils.save_checkpoint(i, args.arch, model, optimizer, compression_scheduler, train_accuracy, False,\n",
    "                                         'hyperopt.trial_{}'.format(count), outputDir)\n",
    "    test_accuracy = test() # Validate hyperparameter setting\n",
    "    score = (1-(val_accuracy/100.)) + (alpha * (1-sparsity/100.)) # objective funtion here\n",
    "    \n",
    "    print('{:2d} trials: score: {:.4f}\\ttrain acc:{:.4f}\\tval acc:{:.4f}\\ttest acc:{:.4f}\\tsparsity:{:.4f}'\\\n",
    "          .format(count, score, train_accuracy, val_accuracy, test_accuracy, sparsity))\n",
    "    f.write('{},{:.4f},{:.4f},{:.4f},{:.4f},{:.4f}\\n'\\\n",
    "          .format(count, score, train_accuracy, val_accuracy, test_accuracy, sparsity))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: TPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. No Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    space = get_space()\n",
    "    best = fmin(objective, space, algo=hyperopt.tpe.suggest, max_evals=args.rounds)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDir = os.path.join('./logs', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "os.mkdir(outputDir)\n",
    "\n",
    "f = open(os.path.join(outputDir, 'trials.log'), 'w')       \n",
    "f.write(\"trial,score,train acc,val acc,sparsity\\n\")\n",
    "count = 0\n",
    "best = main()\n",
    "f.close()\n",
    "\n",
    "with open(os.path.join(outputDir, 'best.json'), 'w') as f:       \n",
    "    f.write(str(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. No Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    space = get_space()\n",
    "    best = fmin(objective, space, algo=hyperopt.rand.suggest, max_evals=args.rounds)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 trials: score: 0.2980\ttrain acc:87.4022\tval acc:81.0600\ttest acc:82.9300\tsparsity:45.6980\n",
      " 2 trials: score: 0.2624\ttrain acc:86.3222\tval acc:81.5000\ttest acc:82.4600\tsparsity:61.2892\n",
      " 3 trials: score: 0.2618\ttrain acc:88.6511\tval acc:83.0600\ttest acc:83.8100\tsparsity:53.8223\n",
      " 4 trials: score: 0.2642\ttrain acc:87.9000\tval acc:81.7800\ttest acc:82.9200\tsparsity:59.0169\n",
      " 5 trials: score: 0.2873\ttrain acc:87.9267\tval acc:81.6200\ttest acc:83.7100\tsparsity:48.2540\n",
      " 6 trials: score: 0.2895\ttrain acc:86.8978\tval acc:81.0600\ttest acc:82.5900\tsparsity:49.9257\n",
      " 7 trials: score: 0.2744\ttrain acc:88.3978\tval acc:82.4800\ttest acc:83.7200\tsparsity:50.3904\n",
      " 8 trials: score: 0.2725\ttrain acc:87.2822\tval acc:82.7800\ttest acc:83.2500\tsparsity:49.8620\n",
      " 9 trials: score: 0.2644\ttrain acc:87.7444\tval acc:82.4200\ttest acc:83.4000\tsparsity:55.7215\n",
      "10 trials: score: 0.2633\ttrain acc:86.8822\tval acc:82.2400\ttest acc:82.8400\tsparsity:57.1688\n"
     ]
    }
   ],
   "source": [
    "outputDir = os.path.join('./logs', datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "os.mkdir(outputDir)\n",
    "\n",
    "f = open(os.path.join(outputDir, 'trials.log'), 'w')       \n",
    "f.write(\"trial,score,train acc,val acc,sparsity\\n\")\n",
    "count = 0\n",
    "best = main()\n",
    "f.close()\n",
    "\n",
    "with open(os.path.join(outputDir, 'best.json'), 'w') as f:       \n",
    "    f.write(str(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distiller",
   "language": "python",
   "name": "distiller"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
