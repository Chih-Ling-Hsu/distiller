{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Examine Model Size\n",
    "\n",
    "parameters only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import subprocess\n",
    "import torch\n",
    "import pickle as pkl\n",
    "\n",
    "# Load some common jupyter code\n",
    "%run distiller_jupyter_helpers.ipynb\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive, interact, Layout\n",
    "from models import create_model\n",
    "from apputils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_files = {\n",
    "    'Original': '../resnet20_cifar_baseline/best.pth.tar',\n",
    "    'Pruning_filter-wise': '../examples/classifier_compression/logs/2018.11.03-231903/best.pth.tar',\n",
    "    'Pruning_element-wise': '../examples/classifier_compression/logs/2018.11.03-215240/best.pth.tar',\n",
    "    'Quantization': '../examples/classifier_compression/logs/2018.11.04-145420/best.pth.tar'\n",
    "}\n",
    "\n",
    "model_dict = {\n",
    "    'Original': create_model(False, 'cifar10', 'resnet20_cifar'),\n",
    "    'Pruning_filter-wise': create_model(False, 'cifar10', 'resnet20_cifar'),\n",
    "    'Pruning_element-wise': create_model(False, 'cifar10', 'resnet20_cifar'),\n",
    "    'Quantization': create_model(False, 'cifar10', 'resnet20_cifar')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ckpt_files:\n",
    "    load_checkpoint(model_dict[k], ckpt_files[k]);\n",
    "    torch.save(model_dict[k].module, '{}.pt'.format(k))\n",
    "    #model_dict[k].module.save_state_dict('{}.pt'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'distiller.quantization.clipped_linear.DorefaQuantizer'>\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('../examples/classifier_compression/logs/2018.11.04-145420/best.pth.tar')\n",
    "qmd = checkpoint['quantizer_metadata']\n",
    "print(qmd['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_dict['Quantization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['module.layer1.0.conv1.float_weight',\n",
       " 'module.layer1.0.conv2.float_weight',\n",
       " 'module.layer1.1.conv1.float_weight',\n",
       " 'module.layer1.1.conv2.float_weight',\n",
       " 'module.layer1.2.conv1.float_weight',\n",
       " 'module.layer1.2.conv2.float_weight',\n",
       " 'module.layer2.0.conv1.float_weight',\n",
       " 'module.layer2.0.conv2.float_weight',\n",
       " 'module.layer2.0.downsample.0.float_weight',\n",
       " 'module.layer2.1.conv1.float_weight',\n",
       " 'module.layer2.1.conv2.float_weight',\n",
       " 'module.layer2.2.conv1.float_weight',\n",
       " 'module.layer2.2.conv2.float_weight',\n",
       " 'module.layer3.0.conv1.float_weight',\n",
       " 'module.layer3.0.conv2.float_weight',\n",
       " 'module.layer3.0.downsample.0.float_weight',\n",
       " 'module.layer3.1.conv1.float_weight',\n",
       " 'module.layer3.1.conv2.float_weight',\n",
       " 'module.layer3.2.conv1.float_weight',\n",
       " 'module.layer3.2.conv2.float_weight']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in model.state_dict().keys() if 'float' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [model.state_dict()[k] for k in model.state_dict() if ('tracked' not in k) and \\\n",
    "                                                     ('float' not in k) and ('running' not in k)]\n",
    "with open('Quantization.pt', 'wb') as f:\n",
    "    pkl.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 dllab users 1117K Nov  4 17:44 Original.pt\r\n",
      "-rw-r--r--  1 dllab users  904K Nov  4 17:44 Pruning_element-wise.pt\r\n",
      "-rw-r--r--  1 dllab users  803K Nov  4 17:44 Pruning_filter-wise.pt\r\n",
      "-rw-r--r--  1 dllab users 1084K Nov  4 17:45 Quantization.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah --block-size=K | grep .pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original MACs/sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-082247/2018.11.04-082247.log\n",
      "==> using cifar10 dataset\n",
      "=> creating resnet20_cifar model for CIFAR10\n",
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n",
      "=> loading checkpoint ../../resnet20_cifar_baseline/best.pth.tar\n",
      "Checkpoint keys:\n",
      "epoch\n",
      "\tarch\n",
      "\tstate_dict\n",
      "\tbest_top1\n",
      "\toptimizer\n",
      "\tcompression_sched\n",
      "   best top@1: 91.530\n",
      "Loaded compression schedule from checkpoint (epoch 299)\n",
      "=> loaded checkpoint '../../resnet20_cifar_baseline/best.pth.tar' (epoch 299)\n",
      "Optimizer Type: <class 'torch.optim.sgd.SGD'>\n",
      "Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}\n",
      "\n",
      "Parameters:\n",
      "+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+\n",
      "|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |\n",
      "|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|\n",
      "|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            432 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.21644 | -0.00101 |    0.14352 |\n",
      "|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           2304 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.07648 | -0.00313 |    0.04473 |\n",
      "|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           2304 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.07466 |  0.00082 |    0.05079 |\n",
      "|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           2304 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.06451 | -0.00615 |    0.04385 |\n",
      "|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |           2304 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.06168 | -0.00228 |    0.04048 |\n",
      "|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           2304 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.07594 | -0.00953 |    0.05307 |\n",
      "|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |           2304 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.06665 | -0.00645 |    0.04689 |\n",
      "|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           4608 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.07751 | -0.00258 |    0.05791 |\n",
      "|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           9216 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.06670 | -0.00337 |    0.05074 |\n",
      "|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            512 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.15936 |  0.00005 |    0.09787 |\n",
      "| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           9216 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.06068 | -0.00207 |    0.04584 |\n",
      "| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           9216 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.05372 | -0.00654 |    0.04231 |\n",
      "| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           9216 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.06479 | -0.00612 |    0.04963 |\n",
      "| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           9216 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.05188 | -0.00365 |    0.04062 |\n",
      "| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |          18432 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.06171 | -0.00455 |    0.04878 |\n",
      "| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          36864 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.05749 | -0.00298 |    0.04559 |\n",
      "| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |           2048 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.07436 | -0.00420 |    0.05678 |\n",
      "| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          36864 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.05539 | -0.00666 |    0.04391 |\n",
      "| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          36864 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.03909 | -0.00319 |    0.03033 |\n",
      "| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          36864 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.03433 | -0.00677 |    0.02695 |\n",
      "| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          36864 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.01979 | -0.00003 |    0.01362 |\n",
      "| 21 | module.fc.weight                    | (10, 64)       |           640 |            640 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.39841 | -0.00001 |    0.28145 |\n",
      "| 22 | Total sparsity:                     | -              |        270896 |         270896 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    0.00000 | 0.00000 |  0.00000 |    0.00000 |\n",
      "+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+\n",
      "Total sparsity: 0.00\n",
      "\n",
      "\n",
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-082247/2018.11.04-082247.log\n"
     ]
    }
   ],
   "source": [
    "!python ../examples/classifier_compression/compress_classifier.py \\\n",
    "                --arch resnet20_cifar /tmp/dataset-nctu -p=50 \\\n",
    "                --resume=../../resnet20_cifar_baseline/best.pth.tar \\\n",
    "                --summary sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-082255/2018.11.04-082255.log\n",
      "==> using cifar10 dataset\n",
      "=> creating resnet20_cifar model for CIFAR10\n",
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n",
      "=> loading checkpoint ../../resnet20_cifar_baseline/best.pth.tar\n",
      "Checkpoint keys:\n",
      "epoch\n",
      "\tarch\n",
      "\tstate_dict\n",
      "\tbest_top1\n",
      "\toptimizer\n",
      "\tcompression_sched\n",
      "   best top@1: 91.530\n",
      "Loaded compression schedule from checkpoint (epoch 299)\n",
      "=> loaded checkpoint '../../resnet20_cifar_baseline/best.pth.tar' (epoch 299)\n",
      "Optimizer Type: <class 'torch.optim.sgd.SGD'>\n",
      "Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}\n",
      "+----+-----------------------+--------+----------+-----------------+--------------+-----------------+--------------+------------------+---------+\n",
      "|    | Name                  | Type   | Attrs    | IFM             |   IFM volume | OFM             |   OFM volume |   Weights volume |    MACs |\n",
      "|----+-----------------------+--------+----------+-----------------+--------------+-----------------+--------------+------------------+---------|\n",
      "|  0 | conv1                 | Conv2d | k=(3, 3) | (1, 3, 32, 32)  |         3072 | (1, 16, 32, 32) |        16384 |              432 |  442368 |\n",
      "|  1 | layer1.0.conv1        | Conv2d | k=(3, 3) | (1, 16, 32, 32) |        16384 | (1, 16, 32, 32) |        16384 |             2304 | 2359296 |\n",
      "|  2 | layer1.0.conv2        | Conv2d | k=(3, 3) | (1, 16, 32, 32) |        16384 | (1, 16, 32, 32) |        16384 |             2304 | 2359296 |\n",
      "|  3 | layer1.1.conv1        | Conv2d | k=(3, 3) | (1, 16, 32, 32) |        16384 | (1, 16, 32, 32) |        16384 |             2304 | 2359296 |\n",
      "|  4 | layer1.1.conv2        | Conv2d | k=(3, 3) | (1, 16, 32, 32) |        16384 | (1, 16, 32, 32) |        16384 |             2304 | 2359296 |\n",
      "|  5 | layer1.2.conv1        | Conv2d | k=(3, 3) | (1, 16, 32, 32) |        16384 | (1, 16, 32, 32) |        16384 |             2304 | 2359296 |\n",
      "|  6 | layer1.2.conv2        | Conv2d | k=(3, 3) | (1, 16, 32, 32) |        16384 | (1, 16, 32, 32) |        16384 |             2304 | 2359296 |\n",
      "|  7 | layer2.0.conv1        | Conv2d | k=(3, 3) | (1, 16, 32, 32) |        16384 | (1, 32, 16, 16) |         8192 |             4608 | 1179648 |\n",
      "|  8 | layer2.0.conv2        | Conv2d | k=(3, 3) | (1, 32, 16, 16) |         8192 | (1, 32, 16, 16) |         8192 |             9216 | 2359296 |\n",
      "|  9 | layer2.0.downsample.0 | Conv2d | k=(1, 1) | (1, 16, 32, 32) |        16384 | (1, 32, 16, 16) |         8192 |              512 |  131072 |\n",
      "| 10 | layer2.1.conv1        | Conv2d | k=(3, 3) | (1, 32, 16, 16) |         8192 | (1, 32, 16, 16) |         8192 |             9216 | 2359296 |\n",
      "| 11 | layer2.1.conv2        | Conv2d | k=(3, 3) | (1, 32, 16, 16) |         8192 | (1, 32, 16, 16) |         8192 |             9216 | 2359296 |\n",
      "| 12 | layer2.2.conv1        | Conv2d | k=(3, 3) | (1, 32, 16, 16) |         8192 | (1, 32, 16, 16) |         8192 |             9216 | 2359296 |\n",
      "| 13 | layer2.2.conv2        | Conv2d | k=(3, 3) | (1, 32, 16, 16) |         8192 | (1, 32, 16, 16) |         8192 |             9216 | 2359296 |\n",
      "| 14 | layer3.0.conv1        | Conv2d | k=(3, 3) | (1, 32, 16, 16) |         8192 | (1, 64, 8, 8)   |         4096 |            18432 | 1179648 |\n",
      "| 15 | layer3.0.conv2        | Conv2d | k=(3, 3) | (1, 64, 8, 8)   |         4096 | (1, 64, 8, 8)   |         4096 |            36864 | 2359296 |\n",
      "| 16 | layer3.0.downsample.0 | Conv2d | k=(1, 1) | (1, 32, 16, 16) |         8192 | (1, 64, 8, 8)   |         4096 |             2048 |  131072 |\n",
      "| 17 | layer3.1.conv1        | Conv2d | k=(3, 3) | (1, 64, 8, 8)   |         4096 | (1, 64, 8, 8)   |         4096 |            36864 | 2359296 |\n",
      "| 18 | layer3.1.conv2        | Conv2d | k=(3, 3) | (1, 64, 8, 8)   |         4096 | (1, 64, 8, 8)   |         4096 |            36864 | 2359296 |\n",
      "| 19 | layer3.2.conv1        | Conv2d | k=(3, 3) | (1, 64, 8, 8)   |         4096 | (1, 64, 8, 8)   |         4096 |            36864 | 2359296 |\n",
      "| 20 | layer3.2.conv2        | Conv2d | k=(3, 3) | (1, 64, 8, 8)   |         4096 | (1, 64, 8, 8)   |         4096 |            36864 | 2359296 |\n",
      "| 21 | fc                    | Linear |          | (1, 64)         |           64 | (1, 10)         |           10 |              640 |     640 |\n",
      "+----+-----------------------+--------+----------+-----------------+--------------+-----------------+--------------+------------------+---------+\n",
      "Total MACs: 40,813,184\n",
      "\n",
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-082255/2018.11.04-082255.log\n"
     ]
    }
   ],
   "source": [
    "!python ../examples/classifier_compression/compress_classifier.py \\\n",
    "                --arch resnet20_cifar /tmp/dataset-nctu -p=50 \\\n",
    "                --resume=../../resnet20_cifar_baseline/best.pth.tar \\\n",
    "                --summary compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Examine Model Speed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for k in ckpt_files:\n",
    "    output_dict[k] = subprocess.check_output('''time python ../examples/classifier_compression/compress_classifier.py \n",
    "                        --arch resnet20_cifar /tmp/dataset-nctu -p=50 --resume={} --evaluate'''{ckpt_files[k]}, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-174839/2018.11.04-174839.log\n",
      "==> using cifar10 dataset\n",
      "=> creating resnet20_cifar model for CIFAR10\n",
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n",
      "=> loading checkpoint ../../resnet20_cifar_baseline/best.pth.tar\n",
      "Checkpoint keys:\n",
      "epoch\n",
      "\tarch\n",
      "\tstate_dict\n",
      "\tbest_top1\n",
      "\toptimizer\n",
      "\tcompression_sched\n",
      "   best top@1: 91.530\n",
      "Loaded compression schedule from checkpoint (epoch 299)\n",
      "=> loaded checkpoint '../../resnet20_cifar_baseline/best.pth.tar' (epoch 299)\n",
      "Optimizer Type: <class 'torch.optim.sgd.SGD'>\n",
      "Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset sizes:\n",
      "\ttraining=45000\n",
      "\tvalidation=5000\n",
      "\ttest=10000\n",
      "--- test ---------------------\n",
      "10000 samples (256 per mini-batch)\n",
      "==> Top1: 91.530    Top5: 99.640    Loss: 0.542\n",
      "\n",
      "--- 1.0533514022827148 seconds ---\n",
      "\n",
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-174839/2018.11.04-174839.log\n"
     ]
    }
   ],
   "source": [
    "!python ../examples/classifier_compression/compress_classifier.py \\\n",
    "                --arch resnet20_cifar /tmp/dataset-nctu -p=50 \\\n",
    "                --resume=../../resnet20_cifar_baseline/best.pth.tar \\\n",
    "                --evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-174850/2018.11.04-174850.log\n",
      "==> using cifar10 dataset\n",
      "=> creating resnet20_cifar model for CIFAR10\n",
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n",
      "=> loading checkpoint ../examples/classifier_compression/logs/2018.11.03-231903/best.pth.tar\n",
      "Checkpoint keys:\n",
      "epoch\n",
      "\tarch\n",
      "\tstate_dict\n",
      "\tbest_top1\n",
      "\toptimizer\n",
      "\tcompression_sched\n",
      "\tthinning_recipes\n",
      "   best top@1: 90.220\n",
      "Loaded compression schedule from checkpoint (epoch 434)\n",
      "Loaded a thinning recipe from the checkpoint\n",
      "Executed 1 recipes\n",
      "=> loaded checkpoint '../examples/classifier_compression/logs/2018.11.03-231903/best.pth.tar' (epoch 434)\n",
      "Optimizer Type: <class 'torch.optim.sgd.SGD'>\n",
      "Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset sizes:\n",
      "\ttraining=45000\n",
      "\tvalidation=5000\n",
      "\ttest=10000\n",
      "--- test ---------------------\n",
      "10000 samples (256 per mini-batch)\n",
      "==> Top1: 90.220    Top5: 99.640    Loss: 0.430\n",
      "\n",
      "--- 1.0105717182159424 seconds ---\n",
      "\n",
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-174850/2018.11.04-174850.log\n"
     ]
    }
   ],
   "source": [
    "!python ../examples/classifier_compression/compress_classifier.py \\\n",
    "                --arch resnet20_cifar /tmp/dataset-nctu -p=50 \\\n",
    "                --resume=../examples/classifier_compression/logs/2018.11.03-231903/best.pth.tar \\\n",
    "                --evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-174901/2018.11.04-174901.log\n",
      "==> using cifar10 dataset\n",
      "=> creating resnet20_cifar model for CIFAR10\n",
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n",
      "=> loading checkpoint ../examples/classifier_compression/logs/2018.11.03-215240/best.pth.tar\n",
      "Checkpoint keys:\n",
      "epoch\n",
      "\tarch\n",
      "\tstate_dict\n",
      "\tbest_top1\n",
      "\toptimizer\n",
      "\tcompression_sched\n",
      "\tthinning_recipes\n",
      "   best top@1: 90.160\n",
      "Loaded compression schedule from checkpoint (epoch 405)\n",
      "Loaded a thinning recipe from the checkpoint\n",
      "Executed 1 recipes\n",
      "=> loaded checkpoint '../examples/classifier_compression/logs/2018.11.03-215240/best.pth.tar' (epoch 405)\n",
      "Optimizer Type: <class 'torch.optim.sgd.SGD'>\n",
      "Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset sizes:\n",
      "\ttraining=45000\n",
      "\tvalidation=5000\n",
      "\ttest=10000\n",
      "--- test ---------------------\n",
      "10000 samples (256 per mini-batch)\n",
      "==> Top1: 90.160    Top5: 99.650    Loss: 0.395\n",
      "\n",
      "--- 1.0004069805145264 seconds ---\n",
      "\n",
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-174901/2018.11.04-174901.log\n"
     ]
    }
   ],
   "source": [
    "!python ../examples/classifier_compression/compress_classifier.py \\\n",
    "                --arch resnet20_cifar /tmp/dataset-nctu -p=50 \\\n",
    "                --resume=../examples/classifier_compression/logs/2018.11.03-215240/best.pth.tar \\\n",
    "                --evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-174911/2018.11.04-174911.log\n",
      "==> using cifar10 dataset\n",
      "=> creating resnet20_cifar model for CIFAR10\n",
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n",
      "=> loading checkpoint ../examples/classifier_compression/logs/2018.11.04-145420/best.pth.tar\n",
      "Checkpoint keys:\n",
      "epoch\n",
      "\tarch\n",
      "\tstate_dict\n",
      "\tbest_top1\n",
      "\toptimizer\n",
      "\tcompression_sched\n",
      "\tquantizer_metadata\n",
      "   best top@1: 90.000\n",
      "Loaded compression schedule from checkpoint (epoch 406)\n",
      "Loaded quantizer metadata from the checkpoint\n",
      "Preparing model for quantization using DorefaQuantizer\n",
      "Parameter 'module.layer1.0.conv1.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer1.0.conv2.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer1.1.conv1.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer1.1.conv2.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer1.2.conv1.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer1.2.conv2.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer2.0.conv1.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer2.0.conv2.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer2.0.downsample.0.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer2.1.conv1.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer2.1.conv2.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer2.2.conv1.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer2.2.conv2.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer3.0.conv1.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer3.0.conv2.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer3.0.downsample.0.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer3.1.conv1.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer3.1.conv2.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer3.2.conv1.weight' will be quantized to 8 bits\n",
      "Parameter 'module.layer3.2.conv2.weight' will be quantized to 8 bits\n",
      "Quantized model:\n",
      "\n",
      "DataParallel(\n",
      "  (module): ResNetCifar(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ClippedLinearQuantization(num_bits=4, clip_val=1, inplace)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ClippedLinearQuantization(num_bits=4, clip_val=1)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "    (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "=> loaded checkpoint '../examples/classifier_compression/logs/2018.11.04-145420/best.pth.tar' (epoch 406)\n",
      "Optimizer Type: <class 'torch.optim.sgd.SGD'>\n",
      "Optimizer Args: {'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0001, 'nesterov': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset sizes:\n",
      "\ttraining=45000\n",
      "\tvalidation=5000\n",
      "\ttest=10000\n",
      "--- test ---------------------\n",
      "10000 samples (256 per mini-batch)\n",
      "==> Top1: 89.990    Top5: 99.700    Loss: 0.330\n",
      "\n",
      "--- 1.045027732849121 seconds ---\n",
      "\n",
      "Log file for this run: /home/dllab/distiller/jupyter/logs/2018.11.04-174911/2018.11.04-174911.log\n"
     ]
    }
   ],
   "source": [
    "!python ../examples/classifier_compression/compress_classifier.py \\\n",
    "                --arch resnet20_cifar /tmp/dataset-nctu -p=50 \\\n",
    "                --resume=../examples/classifier_compression/logs/2018.11.04-145420/best.pth.tar \\\n",
    "                --evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distiller",
   "language": "python",
   "name": "distiller"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
